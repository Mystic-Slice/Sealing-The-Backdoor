# Knowledge Distillation

This repository contains the code and sample results related to the Knowledge Distillation technique as part of the project "Mitigating Data Poisoning Attacks on Diffusion Models using Unlearning".

The code files are:
1. teacherstudent.py - Vanilla Knowledge Distillation
2. attention_guided_kd.py - Knowledge Distillation with Attention Guidance (Gaussian Noise matching)
3. attention_guided_kd_black.py - Knowledge Distillation with Attention Guidance (Black Image matching)
4. attention_guided_kd_random_words.py - Knowledge Distillation with Attention Guidance (Random Words matching)
5. finetune_rev.py - Finetune reversal of poisoning

The attention capture mechanism is from https://github.com/wooyeolBaek/attention-map

This project is still a WIP.

